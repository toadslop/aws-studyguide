+++
title="Load Balancer Connectivity Patterns Part 1: Internal"
date=2024-10-17

[taxonomies]
exams = ["Advanced Networking"]
topics = ["network design", "load balancing"]
[extra]
toc = true
comments = true
+++

Hey there, cloud burrowers! Bit the Chipmunk here, digging into one of the trickier corners of AWS networking: **internal load balancers**.

Many developers think â€œload balancer = public entry point.â€
But in real AWS environments, *most* load balancers never touch the internet â€” they live deep inside private networks, keeping traffic organized, secure, and scalable behind the scenes.

<!--more-->

Letâ€™s explore the key **connectivity patterns** youâ€™ll need to recognize on the AWS exam for **internal** load balancers.

---

## ğŸ§  What Makes a Load Balancer â€œInternalâ€?

An internal load balancer isnâ€™t reachable from the internet. It only has **private IPs** and **resolves to private DNS names**.

Youâ€™ll use it to route traffic:

* Between tiers of an application (e.g., web â†’ app â†’ database)
* Between VPCs (via PrivateLink or Peering)
* From on-prem networks through VPN or Direct Connect

**Exam clue:**
If a question says â€œno public endpoint,â€ â€œprivate connectivity,â€ or â€œtraffic must stay within the VPC,â€ the answer probably involves an **internal ALB** or **internal NLB**.

---

## ğŸ§© 1. Single-VPC Internal Load Balancing

This is the simplest internal pattern â€” everything lives in one VPC.

| **Use Case**                      | **Pattern**                             | **Key Points**                                              |
| --------------------------------- | --------------------------------------- | ----------------------------------------------------------- |
| Multi-tier app (web â†’ app â†’ DB)   | **Internal ALB or NLB**                 | Front-end servers send traffic to backend tiers privately.  |
| ECS or EKS service communication  | **Service-to-service via Internal NLB** | Load balances containers or pods within private subnets.    |
| Simple microservices deployments | **Internal ALB + Service Discovery**    | Each service has its own target group; DNS handles routing. |

---

## ğŸŒ‰ 2. Cross-VPC: Shared Service or Consumer Patterns

As organizations grow, itâ€™s common to isolate workloads into separate VPCs (per environment, team, or account).
But what happens when one app in VPC-A needs to access services another in VPC-B â€” privately? That's where cross-VPC patterns come in. These patterns are more complex because we're not just talking about load balancers -- we have to choose the correct network connectivity solution as well.

### Option 1: **VPC Peering + Internal Load Balancer**

**Scenario**: Services in VPC A must be able to communicate with services in VPC B.

**Pattern**: Connect VPCs using VPC peering; front services with a load balancer to improve availability and resiliency

ğŸ’¡ **Key Notes:**

* The consumer VPC must add route table entries to the producerâ€™s subnets via the peering connection.
* If using DNS for routing, DNS resolution must be enabled for private zones or manually configured.

âš ï¸ **Exam trap 1:**
Peering doesnâ€™t scale well â€” no transitive routing. For many-to-one or hub-spoke topologies, look at PrivateLink or Transit Gateway instead.

âš ï¸ **Exam trap 2:**
Peering enables full connectivity between the two VPCs â€” it is not an option if only the services backed by the load balancer should be accessible. If this is a requirement, PrivateLink is the correct pattern.

---

### Option 2: **PrivateLink (Endpoint Service)**

**Scenario**: Services in a VPC must be accessible to services in other VPCs without exposing full network connectivity.

**Common Use Case**: One company provides a SaaS application to other companies; they want to provide access to the service only, without providing full network access to their VPC

**Pattern**: Front services with an NLB and expose them to consumer VPCs via PrivateLink.

ğŸ’¡ **Why PrivateLink Shines:**

* Scales across many consumers.
* Consumers see the service as a private IP in their own subnet.
* Traffic never leaves the AWS backbone.

âš ï¸ **Exam trap:**
Only **NLBs** can be used as PrivateLink *endpoint services* â€” not ALBs.
If a question says â€œservice must be shared privately across accounts,â€ **PrivateLink** is the answer.

---

### Option 3: **Transit Gateway + Internal Load Balancers**

When you have multiple VPCs and networks (including on-prem) that need to interconnect, **AWS Transit Gateway (TGW)** acts as the central hub. Use this pattern when you need more scale than VPC peering can offer.

**Scenario**: Services in a VPC need to be reachable from a large number of other VPCs across a large organization.

**Common Use Case**: Shared services VPC â€” a VPC that hosts common services needed across an organization

ğŸ’¡ **Why This Matters:**

* Simplifies routing â€” you donâ€™t need full mesh peering.
* Enables hybrid routing (DX/VPN to AWS).

âš ï¸ **Exam tip:**
TGW + Internal Load Balancer â‰  PrivateLink.
PrivateLink is *service exposure*; TGW is *network-level connectivity*.

---

## ğŸ¢ 3. Hybrid Connectivity: Bridging AWS and On-Prem

Hybrid load balancing patterns make sure traffic flows smoothly between AWS and on-prem environments â€” securely, privately, and resiliently.

### ğŸ  Pattern 1: AWS â†’ On-Prem Services

**Scenario:**
You have AWS-based workloads (like EC2 apps or Lambda functions) that need to call **on-prem services** â€” maybe a legacy database or authentication service â€” over a **VPN or Direct Connect** link.

**Design Pattern:**
Use an **Internal NLB** in AWS to represent your on-prem endpoints.
The NLB targets are the **private IPs of the on-prem systems**, reachable via DX or VPN.

**Why this works:**

* Keeps routing **simple** â€” AWS resources see a local endpoint (the NLB), not the remote IP.
* Supports **health checks and failover** across multiple on-prem targets.
* Enables **VPC service discovery** (e.g., via PrivateLink or Route 53 Private Hosted Zone).

**Exam Trigger:**

> â€œAWS workloads must connect to on-prem services over Direct Connect using private IPs.â€
> âœ… **Answer:** Internal NLB targeting on-prem IPs.

---

### â˜ï¸ Pattern 2: On-Prem â†’ AWS Services

**Scenario:**
You have **on-prem clients or applications** that access **services hosted in AWS** â€” maybe a REST API or a private web tier â€” over a **private connection (VPN or DX)**.

**Design Pattern:**
Deploy an **Internal ALB or NLB** in AWS to front those AWS services.
On-prem clients resolve a **private DNS name** that maps to the load balancerâ€™s private IPs.

**Why this works:**

* Keeps all traffic **off the public internet**.
* The load balancer provides **high availability** and **multi-AZ failover** inside AWS.
* Simplifies **network policy management** â€” you point all traffic to one endpoint instead of individual EC2 instances.

**Exam Trigger:**

> â€œOn-prem clients must access AWS-hosted services privately via Direct Connect without exposing a public endpoint.â€
> âœ… **Answer:** Internal ALB or NLB fronting the AWS service.

---

### ğŸ§  Bitâ€™s Exam Tip

Remember, in both hybrid patterns:

* Youâ€™re using **private IP addressing** â€” no public DNS or internet gateways involved.
* **Internal load balancers** act as stable endpoints on the AWS side of the connection.
* The key clue in the question is usually **â€œmust use private connectivity (VPN/DX)â€** or **â€œno internet exposure.â€**


---

## ğŸ§± 5. Internal Load Balancers for Service Mesh and Microservices

Modern architectures often use **ECS**, **EKS**, or **App Mesh**, where internal LBs provide stable entry points for services.

| **Pattern**                  | **Platform**                                  | **Use Case**                                          |
| ---------------------------- | --------------------------------------------- | ----------------------------------------------------- |
| ECS with multiple services   | ALB with multiple listeners and target groups | Layer 7 routing between services.                     |
| EKS with ingress controllers | ALB or NLB managed by controller              | Ingress for Kubernetes workloads.                     |
| App Mesh with NLB endpoints  | NLB as mesh entry point                       | Consistent TCP routing in microservice architectures. |

ğŸ’¡ **Exam Clue:**
When the question says *â€œservices discover each other via DNSâ€* or *â€œcontainerized workloads scale automaticallyâ€*, internal LBs are often in play â€” usually managed automatically by ECS/EKS controllers. We'll talk more about these scenarios in future articles.

---

## ğŸ§© 6. Internal Load Balancers in Multi-Region Designs

Internal LBs donâ€™t talk directly to the internet, but you can still create **multi-region private architectures** â€” for example, using **VPC peering across regions** or **Transit Gateway inter-region peering**.

| **Pattern**                                          | **Use Case**                             | **Exam Clue**                                    |
| ---------------------------------------------------- | ---------------------------------------- | ------------------------------------------------ |
| Internal ALBs per region + Route 53 weighted routing | Private multi-region HA                  | â€œMust fail over privately across regions.â€       |
| Shared internal APIs in each region                  | PrivateLink endpoint services per region | â€œPrivate access from other accounts or regions.â€ |

ğŸ’¡ **Key takeaway:**
DNS-based failover still works for private endpoints â€” as long as you manage the Route 53 health checks and routing policies correctly.

---

## ğŸŒ° Bitâ€™s Final Bite

Internal load balancers are the **backbone** of private AWS networks.
They make hybrid apps scalable, secure, and reliable without ever exposing a single public IP.

When you see â€œprivate,â€ â€œno internet,â€ or â€œcross-account access,â€ think **internal ALB**, **internal NLB**, or **PrivateLink** â€” and youâ€™ll stay ahead of those tricky exam questions.
